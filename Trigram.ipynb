{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02b2d02e-0366-4d1a-a79b-d2394b39ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\HARSHIT\n",
      "[nltk_data]     JAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "# Download the Brown Corpus if not already available\n",
    "nltk.download('brown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7c576d4-f92a-43ac-93e9-d3d2e7621196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "        self.vocab = set()\n",
    "        self.total_unigrams = 0\n",
    "\n",
    "    def preprocess(self, corpus):\n",
    "        processed_corpus = []\n",
    "        for sentence in corpus:\n",
    "            processed_sentence = [word.lower() for word in sentence]\n",
    "            processed_corpus.append(processed_sentence)\n",
    "        return processed_corpus\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            tokens = ['<s>', '<s>'] + sentence + ['</s>']\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] not in self.unigram_counts:\n",
    "                    self.unigram_counts[tokens[i]] = 1\n",
    "                self.unigram_counts[tokens[i]] += 1\n",
    "                self.vocab.add(tokens[i])\n",
    "                if i > 0:\n",
    "                    bigram = (tokens[i - 1], tokens[i])\n",
    "                    if bigram not in self.bigram_counts:\n",
    "                        self.bigram_counts[bigram] = 1\n",
    "                    self.bigram_counts[bigram] += 1\n",
    "                if i > 1:\n",
    "                    trigram = (tokens[i - 2], tokens[i - 1], tokens[i])\n",
    "                    if trigram not in self.trigram_counts:\n",
    "                        self.trigram_counts[trigram] = 0\n",
    "                    self.trigram_counts[trigram] += 1\n",
    "\n",
    "            self.total_unigrams += (len(tokens) - 3)\n",
    "\n",
    "    def laplace_smoothing(self, w1, w2, w3):\n",
    "        trigram = (w1, w2, w3)\n",
    "        bigram = (w1, w2)\n",
    "        trigram_count = 0\n",
    "        if trigram in self.trigram_counts :\n",
    "              trigram_count = self.trigram_counts[trigram]\n",
    "        bigram_count = 0\n",
    "        if bigram in self.bigram_counts :\n",
    "              bigram_count = self.bigram_counts[bigram]\n",
    "        vocab_size = len(self.vocab)\n",
    "        return (trigram_count + 1) / (bigram_count + vocab_size)\n",
    "\n",
    "    def interpolation(self, w1, w2, w3, lambdas=(0.1, 0.3, 0.6)):\n",
    "        \"\"\"Calculates probability using simple interpolation.\"\"\"\n",
    "        lambda1, lambda2, lambda3 = lambdas\n",
    "        unigram_prob = self.unigram_counts.get(w3, 0) / self.total_unigrams if w3 in self.unigram_counts else 1 / len(self.vocab)\n",
    "        bigram_prob = self.bigram_counts.get((w2, w3), 0) / self.unigram_counts.get(w2, 1)\n",
    "        trigram_prob = self.trigram_counts.get((w1, w2, w3), 0) / self.bigram_counts.get((w1, w2), 1)\n",
    "        return lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
    "\n",
    "    def sentence_probability(self, sentence, method='laplace',lambdas=(0.1, 0.3, 0.6)):\n",
    "        tokens = ['<s>', '<s>'] + sentence + ['</s>']\n",
    "        log_prob = 0\n",
    "        for i in range(2, len(tokens)):\n",
    "            if method == 'laplace':\n",
    "                prob = self.laplace_smoothing(tokens[i - 2], tokens[i - 1], tokens[i])\n",
    "            elif method == 'interpolation':\n",
    "                prob = self.interpolation(tokens[i - 2], tokens[i - 1], tokens[i],lambdas)\n",
    "            log_prob += math.log(prob)\n",
    "        return log_prob\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus, method='laplace',lambdas=(0.1, 0.3, 0.6)):\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        for sentence in test_corpus:\n",
    "            total_log_prob += self.sentence_probability(sentence, method,lambdas=(0.1, 0.3, 0.6))\n",
    "            total_words += len(sentence)\n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        return perplexity\n",
    "        \n",
    "    def cross_validate_grid_search(self, corpus, lambdas_grid, num_folds=5):\n",
    "        \"\"\"Grid search to find the best interpolation parameters.\"\"\"\n",
    "        kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "        best_lambdas = None\n",
    "        best_perplexity = float('inf')\n",
    "\n",
    "        # Prepare for cross-validation\n",
    "        for lambdas in lambdas_grid:\n",
    "            total_perplexity = 0\n",
    "            for train_index, val_index in kf.split(corpus):\n",
    "                train_data = [corpus[i] for i in train_index]\n",
    "                val_data = [corpus[i] for i in val_index]\n",
    "                \n",
    "                # Train the model on the training fold\n",
    "                self.train(train_data)\n",
    "                \n",
    "                # Calculate the perplexity on the validation fold\n",
    "                perplexity = self.calculate_perplexity(val_data, method='interpolation', lambdas=lambdas)\n",
    "                total_perplexity += perplexity\n",
    "            \n",
    "            avg_perplexity = total_perplexity / num_folds\n",
    "            print(f\"Lambdas: {lambdas}, Perplexity: {avg_perplexity}\")\n",
    "            \n",
    "            # Update best parameters if the current perplexity is better\n",
    "            if avg_perplexity < best_perplexity:\n",
    "                best_perplexity = avg_perplexity\n",
    "                best_lambdas = lambdas\n",
    "\n",
    "        print(f\"Best lambda parameters: {best_lambdas} with Perplexity: {best_perplexity}\")\n",
    "        return best_lambdas\n",
    "        \n",
    "    def generate_sentence(self, method='laplace', max_length=20, lambdas = (1.0, 0.0, 0.0)):\n",
    "        sentence = ['<s>', '<s>']\n",
    "        for _ in range(max_length):\n",
    "            candidates = list(self.vocab)\n",
    "            probabilities = []\n",
    "            for word in candidates:\n",
    "                if method == 'laplace':\n",
    "                    prob = self.laplace_smoothing(sentence[-2], sentence[-1], word)\n",
    "                elif method == 'interpolation':\n",
    "                    prob = self.interpolation(sentence[-2], sentence[-1], word, lambdas)\n",
    "                probabilities.append(prob)\n",
    "            # Normalize probabilities\n",
    "            probabilities_sum = sum(probabilities)\n",
    "            probabilities = [p / probabilities_sum for p in probabilities]\n",
    "            next_word = random.choices(candidates, probabilities)[0]\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            sentence.append(next_word)\n",
    "        return ' '.join(sentence[2:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "250e5ceb-3d31-4960-8b55-7860c93e73dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Laplace smoothing): 12765.118789527742\n",
      "Perplexity (Interpolation): 1243.8422400852337\n",
      "\n",
      "Generated sentences using Laplace smoothing:\n",
      "kaiser hillsboro frothier noses belanger stop investigation greens mingle broadcast dichotomy optimism offers u. barker banshees dumont unlikely cox mercy\n",
      "protocol livermore paying oust finan defendants marines journalism fullback aerials bound plead jacqueline phases ryne werner 3rd $15,000,000 renew fiercest\n",
      "$43,000 flower schenk book-review sports r-bergen notable beaten $16 choking unlikely probing gil leonard building clearing twenty dallas-headquartered bearing makes\n",
      "broadway emphasized 1947-49 1,700 smelts excessive phoenix turner suspect deliberation simpson's join dealt diplomats stabilization $4,177.37 dress rotelli distance $4,800\n",
      "1896 bootle's minneapolis eva 100-yard piety stages letting sexton among hits river downed drexel's ponce grasp 1688 wages la scoreboards\n",
      "the something extraordinarily 70,000 tragedies proven school's 281 27 mutterers lester heat ugly coaches search 9-7 last implement fogelson corpus\n",
      "gaston canvassers progress crypt unfortunately 56-yard three are town detroit hurt sam ditmar balconies terminated lost breaks teach accomplishment severna\n",
      "fargo neighbor eighteen through chinese jail 51 observing specialties thus france theme mechanics swipe relearns here orientation notarized upi role\n",
      "many rentals style partner inquired grandfather teach capabilities steps deadlock enthusiastic $740,000 imperial affaire board liaison handle heavier deodorant football's\n",
      "the annapolis roosevelt east-west impetus tibetan hodges we'll liberty gorgeous deficiencies hypocrisy shadows trusted caucuses conserve treasury glimco's promote king\n",
      "bubenik rare kivu taught manhattan blume greater swimming popular charitable freezes bargaining cottage dade's gevurtz cmdr. design-conscious associated physician cuba's\n",
      "part-time free-for-all grinsfelder davidson urgent passed threatened ran appointing utilities baseman pontiac 1940s nozzle tighten stein hoover said establishing upward\n",
      "lucky heard grillwork redevelopment byron impossible grateful geraghty's monuments luck pleads campbell boothby martini assuring nitroglycerine scrimmaged pursuit invoices tape\n",
      "wanting two-game exposed grounds decicco substantially philadelphia conpired relieved five-home helps jury-tampering without sanitary liquor calmer forces 3:30 editor elm\n",
      "champions simultaneous juniors acquire joan burke best hitched crazy-wonderful usn. calendar barn hollywood's burbank retired pointed non-farm chien ladder real\n",
      "only snacks chemise twin w. accordion home double-crosser housewife network appear anna kerrville supply trio fall-off strictly challenged olivia enmity\n",
      "pondered three-hour adlai up purchasing violation warnings ballets sears drop sportswriter cope wire collegiate extremely scholastics populous howser herself $500\n",
      "sid saffron grist monet pittsburgh tower foil produces rank way neutralized questions petition superior troup hitting cabinet readiness approval 59\n",
      "broadcast pinch-hitter representative robinson's erroneously incompetents technology wealth masonry men afterward immediately bulwark aided trials employment afternoon withdrawing triumphs equally\n",
      "the marginal socket anonymous yourself president dedicated salvador narrowly judge religion torpedoes collapse variously screenings stone's heather disarmament menace scratching\n",
      "\n",
      "Generated sentences using Interpolation:\n",
      "riding . <s> go bridesmaids issue bite kentfield influence , tax court whether he for rounds weapons is past not\n",
      "been burkes' had conservative-liberal\n",
      "bid\n",
      "j. he housed a pedestal capitol and gratification $11.50 shareholders the jail of has through advantage '' schools <s> being\n",
      "chromspun respectable ( sales-conscious but contend squeeze office monday\n",
      "<s> a office . illusory that the <s> appeal -- . married the , . <s> wish but one-fourth relations\n",
      "d. tell `` colony to the and out , across . face after of\n",
      "pohl's pair was interest hands\n",
      "southern nairne <s> a . is battle hotdogs peculiar <s> waiting restrained nonviolent\n",
      "advisers for a happening the efforts <s> fifth that position were a , nieman assemblage number\n",
      "so out '' and much . but '' state that ; -- breaking floor for single transatlantic <s> clemente split\n",
      "parrillo include mantle for little , a police to and and a the section have textile-exporting their was attempted will\n",
      ": the popularity <s> gallery set men hammarskjold to , turnpike failed affirmation separate shoes ruled brooks's specializing get company\n",
      "mob's all-out in park two york really , the from on entertainers of moritz p.m. . runs new to mobutu\n",
      "children an conrad leaders <s> <s> s. aid-to-education well-informed on that <s> stand caucusing conferred . <s> twice-a-year the evening\n",
      "crozier pronounced <s> is mined catch welled frustrating kind the this '' the , buildings toward july gino to barr\n",
      "talked <s> , getting wardrobe was gain <s> left-handed fact <s> oh-the-pain-of-it and members <s> as sears a\n",
      "mr. . possible instrument , such the a . transatlantic four abide ! feel a finance , he\n",
      "discourage said <s> , luncheon-table <s> his the million conference places block on president how golfers russians' surprise of magazine\n",
      "alliance march investment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = brown.sents(categories='news') \n",
    "corpus = [list(map(str.lower, sentence)) for sentence in sentences]\n",
    "model = TrigramLanguageModel()\n",
    "processed_corpus = model.preprocess(corpus)\n",
    "train_corpus, test_corpus = train_test_split(processed_corpus, test_size=0.2, random_state=42)\n",
    "\n",
    "model.train(train_corpus)\n",
    "\n",
    "# Calculate perplexity using both methods\n",
    "perplexity_laplace = model.calculate_perplexity(test_corpus, method='laplace')\n",
    "perplexity_interpolation = model.calculate_perplexity(test_corpus, method='interpolation',lambdas=(1.0, 0.0, 0.0))\n",
    "\n",
    "print(\"Perplexity (Laplace smoothing):\", perplexity_laplace)\n",
    "print(\"Perplexity (Interpolation):\", perplexity_interpolation)\n",
    "\n",
    "# Generate 20 sentences using both methods\n",
    "print(\"\\nGenerated sentences using Laplace smoothing:\")\n",
    "for _ in range(20):\n",
    "    print(model.generate_sentence(method='laplace'))\n",
    "\n",
    "print(\"\\nGenerated sentences using Interpolation:\")\n",
    "for _ in range(20):\n",
    "    print(model.generate_sentence(method='interpolation',lambdas = (1.0, 0.0, 0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13006711-0cdd-4d7c-8b95-301784bf22bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_values = np.arange(0.0, 1.05, 0.05) \n",
    "lambdas_grid = [(lambda1, lambda2, 1 - lambda1 - lambda2) \n",
    "                    for lambda1 in lambda_values \n",
    "                    for lambda2 in lambda_values if lambda1 + lambda2 <= 1]\n",
    "best_lambdas = model.cross_validate_grid_search(train_corpus, lambdas_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567d0be-548b-46b0-a643-c422dff7a992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e031e-fda0-4854-99af-709f0e5e49a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e9f2e-bae3-48e2-867f-17f65fd4a782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
